# ========================================================================================================= #
# IMPORTANT - To get started:
# To run the script from the project directory, in R Studio, you need to go to: Tools > Global Options > R Markdown
# and set "Evaluate chunks in directory" to "Project"
# ========================================================================================================= #
# ABOUT
# ========================================================================================================= #
## Brief description: Seamounts raster map downloaded from Pangaea website
### Scripts/processes that needs to be updated prior?:
###
### Script by: 
ff, SGU, R 4.1.1:
### Updated: 
gk, sgu, 20230515, R 4.1.1, update to v2 using new data available 2023
new change is to include a depth weighting to account for higher likelihood of shallow biodiverse rich sea mounts getting impacted
### Developer check: Initial, org, yymmdd, comments
### External check: Initial, org, yymmdd, comments

# ========================================================================================================= # 
# PREPARATIONS
# ========================================================================================================= #

### Install packages
```{r, include = FALSE}
# list all packages that this script is dependent on, remove any redundant ones
x <- c("sf", "raster", "rgdal", "tidyverse", "gdalUtils")
install.packages(x, dependencies=TRUE)
```

### Load packages
```{r, include = FALSE}
library(sf) 
library(raster)
library(rgdal)
library(tidyverse)
library(gdalUtils)
```


## Set version
```{r}
version = "v02" # relates to major iteration of WIOSym (v01 for grid 2021, v02 for grid 2022)
d_version = ".0"  # detailed version (you can add another level e.g. 0.0  -> v01.0.0)
seq = "s01" # sequential order - change accordingly if process is run in several scripts
```

## Set geographic scope
```{r}
read_tsv("./shiny_data_upload/locations.txt")  # prints available locations to choose from and their abbreviations
```

```{r}
location = "reg" # choose applicable 3 letter abbreviation from "location_val" -  reg for whole WIO area
```

## Set main theme
```{r}
folders_raw <- read_tsv("./shiny_data_upload/folders.txt")
folders_data <- read_tsv("./shiny_data_upload/folders_data.txt") # additional folders for data not applicable for data_raw 
folders <- folders_raw %>% add_row(folders_data)
(unique(folders["theme_folder"]))
```

```{r}
theme <- "eco" # Copy from "theme_folder" (e.g. "eco")
```

## Set Subtheme
```{r}
folders %>%
  filter(theme_folder == theme) %>% 
  dplyr::select(subtheme_folder, subtheme_name) %>% 
  print(n = Inf)
```

```{r}
subtheme <- "seamounts" # copy from "subtheme_folder"
```


## Set paths
```{r}
(dest_path <- paste("./data", location, theme, subtheme, version, "", sep="/")) # path to final product
(work_path <- paste(dest_path, "proc/", sep=""))
(proc_path <- paste(dest_path, "proc_log/", sep=""))
(archive_path <- paste(dest_path, "_archive/", sep=""))
(script_path <- "./process/r/")
```

## Create directories
```{r}
dir.create(dest_path, recursive = TRUE)
dir.create(work_path, recursive = TRUE)
dir.create(proc_path, recursive = TRUE)
dir.create(archive_path, recursive = TRUE)
dir.create(script_path, recursive = TRUE)
```

## Save R Script
```{r}
name1 <- "" # "REPLACE_" if script need additional name... don't use unless needed to avoid duplication.
(script_path_windows <- paste(getwd(), "/process/r", sep=""))
(script_name <- paste(theme, "_", subtheme,"_", location, "_", name1, seq, "_", version, ".Rmd", sep=""))
```
 # copy path and name and use File/Save As in R studio to save rmd file
 
#==========================================================================================================================#
# INDATA
#==========================================================================================================================#
# Set DATA_RAW sources
IMPORTANT all data harvested from data_raw needs to have a metasym.txt file in its root directory (not file specific)


## DIR1: Seamounts 2019
```{r}
source1_dir <- source_dir <-  "./data_raw/glo/eco/hab/pangaea/ff2110291355/"  # REPLACE - internal path
source1 <- source_id <- "ff2110291355" # REPLACE
(read_tsv(paste(source_dir, source_id, "_metasym.txt", sep=""))) # check the metasym file
``` 

```{r}
dir(source_dir, recursive=F)
```

```{r}
# Read Files1
source1_path <- path <- paste(source_dir, "YessonEtAl2019-Seamounts-V2/YessonEtAl2019-SeamountBases-V2.shp", sep="")
seamounts <- st_read(path) # REPLACE object_name
# add files from dir as needed
```

## DIR2: Seamounts 2023
```{r}
source2_dir <- source_dir <-  "./data_raw/glo/eco/seamounts/lit/gk2305150903/"  # REPLACE - internal path
source2 <- source_id <- "gk2305150903" # REPLACE
(read_tsv(paste(source_dir, source_id, "_metasym.txt", sep=""))) # check the metasym file
``` 

```{r}
dir(source_dir, recursive=T)
```

```{r}
# Read Files1
source2_path <- path <- paste(source_dir, "SIO_Seamounts/Seamounts_Modeled/all.xyhrdnc", sep="")
seamounts_2023 <- read_delim(path, delim=" ", col_names=F)

names(seamounts_2023) <- c("longitude", "latitude", "height", "radius", "base_depth", "name", "charted")

# add files from dir as needed
```

### declare number of "data_raw" directorys above

```{r}
data_raw_metasym_num <-  2 # set total number of data_raw dir used (e.g. <- 1), to help in export section
```


## DATA sources
IMPORTANT Products in data are based on information in data_raw. Check that *_sourcesym.txt exists for each file to track acccumulated IDs. 
The sourcesym files can be identical within some directory in which case you can load only one, but its not always the case which is why there is one file for each file. 

## DATA1: Grid 1km - loading all flavours (watermask 1, 0, NA) at once since identical sourcesym apply
```{r}
data_dir <-  "./data/reg/grid/grid/v01/"  # input your data directory
dir(data_dir) # check what data is in this directory

data1_file <- "grid_1km_v01.1.tif" 
data1_file2 <- "grid_1km_0_v01.1.tif" 
data1_file3 <- "grid_1km_na_v01.1.tif" 

(grid_1km_path <- path <- paste(data_dir, data1_file,sep=""))
(grid_1km <- raster(path)) # read and check your data
(grid_1km_0_path <- path <- paste(data_dir, data1_file2, sep=""))
(grid_1km_0 <- path <- raster(path)) # read and check your data
(grid_1km_na_path <- path <- paste(data_dir, data1_file3, sep=""))
(grid_1km_na <- path <- raster(path)) # read and check your data
data1_sourcesym <- read_tsv(paste(data_dir, data1_file, "_sourcesym.txt", sep=""))
# check that source IDs exists and make sense

#plot(grid_1km)
```

## DATA2: Grid 250m - loading all flavours (watermask 1, 0, NA) at once since same sourcesym apply
```{r}
data2_dir <- data_dir <-  "./data/reg/grid/grid/v01/"  # input your data directory

data2_file <- "grid_250m_v01.1.tif" # your data file
data2_file2 <- "grid_250m_0_v01.1.tif" # your data file
data2_file3 <- "grid_250m_na_v01.1.tif" # your data file

(grid_250m_path <- path <- paste(data_dir, data2_file, sep=""))
(grid_250m <- raster(path)) # read and check your data
(grid_250m_0_path <- path <- paste(data_dir, data2_file2, sep=""))
(grid_250m_0 <- path <- raster(path)) # read and check your data
(grid_250m_na_path <- path <- paste(data_dir, data2_file3, sep=""))
(grid_250m_na <- path <- raster(path)) # read and check your data
(data2_sourcesym <- read_tsv(paste(data_dir, data2_file, "_sourcesym.txt", sep="")))
# check so source IDs exists and make sense
# check that source IDs exists and make sense
#plot(grid_250m)
```


## DATA 3: bounding box shapefile
```{r}
data3_dir <- data_dir <-  "./data/reg/grid/scope/v01/"  # input your data directory
data3_file <- "wiosym_grid_bounding_box_v01.shp" # your data file
data3_sourcesym <- read_tsv(paste(data_dir, data3_file, "_sourcesym.txt", sep=""))
#data3_sourcesym # check so source IDs exists and make sense - ok for these
data3_path <- data_path <- paste(data_dir, data3_file, sep="")
grid_poly <- st_read(data_path) # read in your data object if appropriate and perhaps change name to more informative...
grid_poly
plot(grid_poly)
```

## DATA4: depth
```{r}
data4_dir <- data_dir <-  "./data/reg/env/topo/v02/"  # input your data directory
dir(data_dir) # check what data is in this directory
data_file <- "bathymetry_mean_m_250m_v02.0.tif" # your data file
(data4_sourcesym <- read_tsv(paste(data_dir, data_file, "_sourcesym.txt", sep="")))
data4_path <- data_path <- paste(data_dir, data_file, sep="")

depth_1km <- raster(data_path) # read in your data object if appropriate and perhaps change name to more informative...
#plot(depth_1km)

# loading additional files of indiviual depth zones created in the bathymetry processing 

data_file1 <- "bathymetry_zone_0-40m_250m_v02.0.tif" 
data_file2 <- "bathymetry_zone_40-200m_250m_v02.0.tif"
data_file3 <- "bathymetry_zone_200-1000m_250m_v02.0.tif"
data_file4 <- "bathymetry_zone_1000m-inf_250m_v02.0.tif"
data_file5 <- "bathymetry_mean_m_250m_v02.0.tif"

data_path1 <- paste(data_dir, data_file1, sep="")
data_path2 <- paste(data_dir, data_file2, sep="")
data_path3 <- paste(data_dir, data_file3, sep="")
data_path4 <- paste(data_dir, data_file4, sep="")
data_path5 <- paste(data_dir, data_file5, sep="")

zone1_r <- raster(data_path1) # shallow 0-40m
zone2_r <- raster(data_path2) # shelf 40-200m
zone3_r <- raster(data_path3) # slope 200-1000m
zone4_r <- raster(data_path4) # abyssal 1000m - inf
depth_250m <- raster(data_path5) # mean depth
```



## declare number of "data" files
```{r}
data_sourcesym_num <-  4 # set total number of data files (e.g. <- 2), to help in export section
```

#==========================================================================================================================#
# PROCESSING
#==========================================================================================================================#

# 2019 Seamount data
## Map seamounts polygons to 250m raster -----------------------------------------------
```{r}
indata_sf <- st_intersection(seamounts, grid_poly) #crop seamounts data to WIOSym area
plot(indata_sf)

source1_path <- path <- paste0(work_path, "YessonEtAl2019-Seamounts-V2_crop.shp")

st_write(indata_sf, source1_path, delete_layer = TRUE)

seamounts1 <- st_read(path)

```

```{r}

# write empty grid for gdalutil work
writeRaster(grid_250m_na, paste(work_path, "grid_250m_seamounts.tif", sep= ""),  overwrite=T, COMPRESS=LZW)
#writeRaster(grid_1km_na, paste(work_path, "grid_1km_seamounts.tif", sep= ""),  overwrite=T, #COMPRESS=LZW)
```


```{r}
# shapefile to 250m grid
sf_warp <- gdalUtils::gdal_rasterize(src_datasource = source1_path,
                                     dst_filename = paste(work_path, "grid_250m_seamounts.tif", sep= ""),
                                     b = 1,
                                     at = F,
                                     a = "Type",
                                     output_Raster = TRUE,
)


summary(sf_warp)
#plot(sf_warp)

```

## change the file name and crs

```{r}
seamounts_250m <- sf_warp
crs(seamounts_250m) <- crs(grid_1km)
(seamounts_250m)

path <- paste(work_path, "seamounts_2019_250m_lzw", sep= "")
writeRaster(seamounts_250m, path,  overwrite=T, COMPRESS=LZW)

seamounts1_250m <- raster(path)
plot(seamounts1_250m)

```

## Aggregate to 1km grid

```{r}
r <- seamounts1_250m
seamounts1_1km <- aggregate(r, fact=4, fun=sum) / 16

r <- seamounts1_1km
# Final check that all is good...
r <- merge(r, grid_1km_0)
r <- crop(r, grid_1km)
r <- mask(r, grid_1km)

#plot(r)
r

seamounts1_1km <- r

path <- paste(work_path, "seamounts_2019_1km", sep= "")
writeRaster(seamounts1_1km, path,  overwrite=T, COMPRESS="LZW")

seamounts1_1km <- raster(path)
plot(seamounts1_1km)

```



# 2023 Seamount data
## buffer seamounts points 
based on radius and map polygons to 250m raster 

```{r}
names(seamounts_2023)
seamounts_2023 <- seamounts_2023 %>% st_as_sf( coords = c("longitude", "latitude")) %>% st_set_crs("+proj=longlat +datum=WGS84")

indata_sf <- st_intersection(seamounts_2023, grid_poly) #crop seamounts data to WIOSym area
plot(indata_sf)

st_write(indata_sf, paste0(work_path, "Seamounts_2023_crop.shp"), delete_layer = TRUE)

```


## buffer seamounts 2023 with radius
```{r}
indata_sf

# transform to metric crs
df <- st_transform(indata_sf, crs = crs(grid_1km))

df <- df %>% mutate(radius=radius*1000)

df <- st_buffer(df, dist = df$radius)

df <- df %>% select(height:charted)

plot(df)

indata_sf_pol <- df 

df <- st_transform(indata_sf_pol, crs = "+proj=longlat +datum=WGS84")

df <- st_cast(df, "MULTIPOLYGON")
#st_geometry(df) <- df$geometry_buffered

# Remove the now redundant geometry_buffered column
#df$geometry_buffered <- NULL

df <- df %>% mutate(Type=1)


path_2023 <- paste0(work_path, "Seamounts_2023_buffered.shp")
#path_2023 <- paste0(work_path, "Seamounts_2023_buffered.gpkg")


st_write(df, path_2023, delete_layer = TRUE)


seamounts2 <- st_read(path_2023)
seamounts1
seamounts2
#CRS(seamounts2) <- crs(grid_1km)
```


```{r}
source1_path <- path <- path_2023

path_dst <- paste(work_path, "grid_250m_seamounts_2023.tif", sep= "")

# write empty grid for gdalutil work
writeRaster(grid_250m_na, path_dst,  overwrite=T, COMPRESS=LZW)

```


```{r}
# GeoPackage to 250m grid
sf_warp <- gdalUtils::gdal_rasterize(src_datasource = source1_path,
                                     dst_filename = path_dst,
                                     b = 1,
                                     at = F,
                                     a = "Type",
                                     output_Raster = TRUE,
)


summary(sf_warp)
#plot(sf_warp)

```
## change the file name and crs

```{r}
seamounts2_250m <- sf_warp
crs(seamounts2_250m) <- crs(grid_1km)
(seamounts2_250m)

path <- paste(work_path, "seamounts_2023_250m_lzw", sep= "")
writeRaster(seamounts2_250m, path,  overwrite=T, COMPRESS="LZW")

seamounts2_250m <- raster(path)
plot(seamounts2_250m)

```

## Aggregate to 1km grid

```{r}
r <- seamounts2_250m
seamounts2_1km <- aggregate(r, fact=4, fun=sum) / 16

r <- seamounts2_1km
# Final check that all is good...
r <- merge(r, grid_1km_0)
r <- crop(r, grid_1km)
r <- mask(r, grid_1km)

#plot(r)
r

seamounts2_1km <- r

path <- paste(work_path, "seamounts_2023_1km", sep= "")
writeRaster(seamounts2_1km, path,  overwrite=T, COMPRESS="LZW")

seamounts2_1km <- raster(path)
plot(seamounts2_1km)

```
# Shallow seamounts weight
```{r}
r <- depth_250m

# shallow seamounts 0-1000m meter
r[r > -1000 ] <- -1000

# very deep seamounts >5000m depth

r[r <= -5000 ] <- -5000

# linear normalisation 0-1 scale
r_norm <- (r+1000)/(-4000)
r_norm1 <- (-1*r_norm)+1 

# transform to 0.25 - 1 scale. 0.25 choosen to give also deep seamounts values and also due to the fact that the depth data is uncertain, many shallower seamounts could be hiding in the depths even if available bathymetry does not show it

# (r1 + k)/x = r2
# => x=4/3, k = 1/4 * 4/3 = 1/3

k = 1/3
x = 4/3

#r1=0
#r1=1
#r2 = (r1 + k)/x

r1 <- r_norm1
r2 = (r1 + k)/x

plot(r2)

```

```{r}
r <- r2
depth_weight_1km <- aggregate(r, fact=4, fun=sum) / 16

r <- depth_weight_1km
# Final check that all is good...
r <- merge(r, grid_1km_0)
r <- crop(r, grid_1km)
r <- mask(r, grid_1km)

#plot(r)
r

depth_weight_1km <- r

path <- paste(work_path, "depth_weight_1km", sep= "")
writeRaster(depth_weight_1km, path,  overwrite=T, COMPRESS="LZW")

depth_weight_1km <- raster(path)
plot(depth_weight_1km)
```

# Merge all Seamount maps, and weight with depth

```{r}
seamount_mosaic <- mosaic(seamounts1_1km, seamounts2_1km, fun=max) 
plot(seamount_mosaic)

seamount_mosaic_w <- seamount_mosaic * depth_weight_1km
plot(seamount_mosaic_w)

path <- paste(work_path, "seamount_mosaic_w_1km", sep= "")
writeRaster(seamount_mosaic_w, path,  overwrite=T, COMPRESS="LZW")
```


#==========================================================================================================================#
# EXPORT
#==========================================================================================================================#
# Write products to root directory when checked and ready to use elsewhere, accompanied by yourfilename.ext_sourcesym.txt 
# for each file to track source IDs as they accumulate between processes / data


# SOURCESYM
### Create sourcesym file with all source IDs used (data and data_raw) 
# data raw sources (dir)
```{r}
print(paste("data_raw metasym files used = ", data_raw_metasym_num, sep=""))
(data_raw_sources <- tibble(id = c(source1, source2))) # REPLACE / add all data_raw sources
```

## data sources (files)
```{r}
print(paste("data_sourcym files = ", data_sourcesym_num, sep=""))
data_sources <- #data1_sourcesym %>% # add any "sourcesym" files applicable to this product (check indata section), keep adding sourcesym files as needed
  #add_row(data2_sourcesym)%>%  
  #add_row(data3_sourcesym)%>% 
  add_row(data4_sourcesym)%>% 
  #add_row(data5_sourcesym)%>% 
  #add_row(data6_sourcesym)%>% 
  unique() %>% 
  print()
```
## Sources combined
```{r}
(all_sources <- data_raw_sources %>% add_row(data_sources))
```
### if a product only use a subset of the total sources, copy the section above and create individual files below (e.g. product1_sources <- ...)


# PRODUCT 1: 
## read objects
```{r}
product_orig <- seamount_mosaic_w # input and check your object to be written to file
#product_norm01 <- REPLACE # input and check your object to be written to file
#product_uncertainty <- REPLACE # input and check your object to be written to file
#plot(c(product_orig))
```
## set names
```{r}
scale <- "1km" # "1km" / "250m"
unit <- "presence_depthweighted"  # add unit for product with original values (e.g. m, km, perc, presence, proportion
component_names <- read_tsv("./shiny_data_upload/component_names.txt") 
component_names_selected <- component_names %>%
  dplyr::filter(theme_folder == theme) %>% 
  dplyr::select(group, file_name, description) %>% 
  print(n = Inf)
```

```{r}
descriptive_name <- "seamounts"  # name of product, for pressure/ecosystem components select from name list above
```
## paths
```{r}
(product_orig_path <- paste(dest_path, descriptive_name, "_", unit, "_", scale, "_", version, d_version, ".tif", sep="" ))
#(product_norm01_path <- paste(product_orig_path, "_uncertainty.tif", sep="" ))
#(product_uncertainty_path <- paste(product_orig_path, "_norm01.tif", sep="" ))
```
## Write to file
```{r}
# check datatype to be appropriate for values (flt or int, 'INT4S' option to..)
writeRaster(product_orig, product_orig_path, COMPRESS=LZW, datatype = 'FLT4S', overwrite=TRUE)
#writeRaster(product_norm01, product_norm01_path, COMPRESS=LZW, datatype = 'FLT4S', overwrite=TRUE)
#writeRaster(product_uncertainty, product_uncertainty_path, COMPRESS=LZW, datatype = 'INT1U', overwrite=TRUE)
# write sourcesym file, change "all_sources" to "selected_sources" if subset only for specific file
write_tsv(all_sources, paste(product_orig_path, "_sourcesym.txt", sep=""))  
#write_tsv(all_sources, paste(product_norm01_path, "_sourcesym.txt", sep=""))
#write_tsv(all_sources, paste(product_uncertainty_path, "_sourcesym.txt", sep=""))
```


# SAVE SCRIPT
## 1. save your current R script (File/Save)
## 2. Save a carbon copy of the R script to proc_log (identical script to what is used to create the products exported
## Do not change the *.Rmd carbon copy in proc_log/ unless you change this version of the products (new versions are developed under process/r)

```{r}
# 2. run code below and go to File/Save As and paste link and name:
dest_path_script <- paste(getwd(), proc_path)
print(dest_path_script <- gsub(" .", "", dest_path_script)) # path
print(script_name_copy <- paste(gsub(".Rmd", "", script_name), d_version, ".Rmd", sep="")) # script name
```

#==========================================================================================================================#
# FINAL CHECK
#==========================================================================================================================#
## Developer check
 Do your own final check on the products and double check sourcesym file exists and are complete for all files in root 
 -> Sign at top of script in about section (# Approved by:)

## External check 
 To ensure repeatability and quality, make sure one colleage can run the script
 When script is proven repeatable/understandable and products/metadata look ok 
 -> Sign at top of script in about section (# Approved by:), and make any comments if needed
 There is also a work_log document in onedrive / gitHUB, check current arrangement with Swedish dev team

